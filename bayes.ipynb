{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadDataset():\n",
    "    postringList =[\n",
    "        ['my','dog','has','flea','problems','help','please'],\n",
    "        ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "        ['my','dalmation','is','so','cute','I','love','him'],\n",
    "        ['stop','posting','stupid','worthless','garbage'],\n",
    "        ['mr','licsk','ate','my','steak','how','to','stop','him'],\n",
    "        ['quit','buying','worthless','dog','food','stupid']\n",
    "    ]\n",
    "    classVec = [0,1,0,1,0,1] # 1은 폭력적인것, 0은 폭력적이지 않은 것\n",
    "    return postringList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set()\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # 합집합을 생성\n",
    "    return list(vocabSet)\n",
    "\n",
    "def sentence2vec(tokens,sentence):\n",
    "    returnVec = [0] * len(tokens) # vccabList 의 길이만큼 0을 가진 vector 생성\n",
    "    for word in sentence:\n",
    "        if word in tokens:\n",
    "            returnVec[tokens.index(word)] = 1\n",
    "        else:\n",
    "            print(\"The word : {} is not in my Vocabulary!\".format(word))\n",
    "    return returnVec\n",
    "\n",
    "def makeDataSet(tokens,sentences):\n",
    "    # return word matrixx\n",
    "    wordMatrix =[]\n",
    "    for sentence in sentences:\n",
    "        wordVec = sentence2vec(tokens,sentence)\n",
    "        wordMatrix.append(wordVec)\n",
    "    return np.array(wordMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dalmation', 'dog', 'buying', 'stop', 'my', 'I', 'quit', 'problems', 'ate', 'cute', 'please', 'flea', 'so', 'food', 'take', 'him', 'to', 'steak', 'stupid', 'love', 'posting', 'maybe', 'worthless', 'has', 'help', 'garbage', 'how', 'not', 'mr', 'licsk', 'is', 'park']\n",
      "[[0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1]\n",
      " [1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0]\n",
      " [0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentences, lables = loadDataset()\n",
    "tokens = createVocabList(sentences)\n",
    "wordVec = sentence2vec(tokens,sentences[0])\n",
    "\n",
    "dataSet = makeDataSet(tokens,sentences)\n",
    "lables = np.array(lables)\n",
    "print(tokens)\n",
    "print(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.4409454422015333, 12.395169719013301]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(dataSet,lables,testVec,k=0.1):\n",
    "    m = dataSet.shape[0]\n",
    "    unique_lable = np.unique(lables)\n",
    "    lable_num = unique_lable.shape[0]\n",
    "    \n",
    "    prob_per_classes = []\n",
    "    Idx2Lable = {}\n",
    "    index = 0\n",
    "    for lable in unique_lable:\n",
    "        Idx2Lable[index] = lable\n",
    "        bool_arr = lables == lable\n",
    "        selected_sentences = dataSet[bool_arr]\n",
    "        classNumber = selected_sentences.shape[0]\n",
    "        prob_Ci = selected_sentences.shape[0] / m\n",
    "        #print(\"Lable : \",lable)\n",
    "        #total_cond_prob = 1\n",
    "        total_cond_prob = 0 # log scale\n",
    "        for attributeIdx in range(dataSet.shape[1]):\n",
    "            value = testVec[attributeIdx]\n",
    "            matched = selected_sentences[:,attributeIdx] # return column vector\n",
    "            selector = matched == value\n",
    "            matched = matched[selector]\n",
    "            count = matched.shape[0]\n",
    "            cond_prob = k + count / 2k + classNumber\n",
    "            \"\"\"\n",
    "            cond_prob = count / classNumber\n",
    "            if cond_prob ==0:\n",
    "                cond_prob = 0.01\n",
    "            \"\"\"\n",
    "            #total_cond_prob *= cond_prob\n",
    "            total_cond_prob += -np.log(cond_prob) # log scale\n",
    "        total_cond_prob *= prob_Ci\n",
    "        prob_per_classes.append(total_cond_prob)\n",
    "        index+=1\n",
    "    idx = np.argmax(prob_per_classes)\n",
    "    idx = np.argmin(prob_per_classes)\n",
    "    print(prob_per_classes)\n",
    "    prediction = Idx2Lable[idx]\n",
    "    return prediction\n",
    "    \n",
    "    \n",
    "testUnit = ['I','love','my','dog']\n",
    "#testUnit =['stupid','dog']\n",
    "testVec = sentence2vec(tokens,testUnit)\n",
    "testVec = np.array(testVec)\n",
    "classify(dataSet,lables,testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
